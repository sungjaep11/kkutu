import requests
from bs4 import BeautifulSoup
import json
import urllib.parse
import os
import time

# ì €ì¥í•  í´ë” ì´ë¦„
save_dir = "long_words"

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs(save_dir, exist_ok=True)

initials = ['ã„±','ã„´','ã„·','ã„¹','ã…','ã…‚','ã……','ã…‡','ã…ˆ','ã…Š','ã…‹','ã…Œ','ã…','ã…']
base_url = "https://kkukowiki.kr/w/ê¸´_ë‹¨ì–´/í•œêµ­ì–´/"
headers = {"User-Agent": "Mozilla/5.0"}

for ch in initials:
    url = base_url + urllib.parse.quote(ch)
    print(f"ğŸ“¥ {ch} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘: {url}")

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ {ch} í˜ì´ì§€ ì‹¤íŒ¨: {e}")
        continue

    soup = BeautifulSoup(response.text, "html.parser")
    tables = soup.find_all("table")

    words = []

    for table in tables:
        headers_th = table.find_all("th")
        if not headers_th:
            continue

        header_texts = [th.get_text(strip=True) for th in headers_th]
        if "ë‹¨ì–´" not in header_texts:
            continue

        word_col_index = header_texts.index("ë‹¨ì–´")
        rows = table.find_all("tr")[1:]

        for row in rows:
            cols = row.find_all("td")
            if len(cols) > word_col_index:
                word = cols[word_col_index].get_text(strip=True)
                if word:
                    words.append(word)

    # íŒŒì¼ ê²½ë¡œ: long_words/words_ã„±.json ë“±
    filename = os.path.join(save_dir, f"words_{ch}.json")
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(words, f, ensure_ascii=False, indent=2)

    print(f"âœ… {ch}: {len(words)}ê°œ ë‹¨ì–´ ì €ì¥ë¨ â†’ {filename}")

    time.sleep(1)  # ì„œë²„ ë³´í˜¸ë¥¼ ìœ„í•œ ëŒ€ê¸°
