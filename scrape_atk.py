import requests
from bs4 import BeautifulSoup
import json
import urllib.parse
import time

initials = ['ã„±','ã„´','ã„·','ã„¹','ã…','ã…‚','ã……','ã…‡','ã…ˆ','ã…Š','ã…‹','ã…Œ','ã…','ã…']

base_urls = [
    "https://kkukowiki.kr/w/ê³µê²©ë‹¨ì–´/í•œêµ­ì–´/",
    "https://kkukowiki.kr/w/ë°©ì–´ë‹¨ì–´/í•œêµ­ì–´/"
]

headers = {"User-Agent": "Mozilla/5.0"}
all_words = []

for base_url in base_urls:
    for ch in initials:
        url = base_url + urllib.parse.quote(ch)
        print(f"ğŸ“¥ í¬ë¡¤ë§ ì¤‘: {url}")
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ ì‹¤íŒ¨: {url} - {e}")
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        tables = soup.find_all("table")

        for table in tables:
            headers_th = table.find_all("th")
            if not headers_th:
                continue

            header_texts = [th.get_text(strip=True) for th in headers_th]
            if "ë‹¨ì–´" not in header_texts:
                continue

            word_col_index = header_texts.index("ë‹¨ì–´")
            rows = table.find_all("tr")[1:]

            for row in rows:
                cols = row.find_all("td")
                if len(cols) > word_col_index:
                    word = cols[word_col_index].get_text(strip=True)
                    if word:
                        all_words.append(word)

        time.sleep(1)  # ì„œë²„ì— ë¶€ë‹´ì„ ì¤„ì´ê¸° ìœ„í•œ ëŒ€ê¸°

# âœ… ì¤‘ë³µ ì œê±° (ë“±ì¥ ìˆœì„œ ìœ ì§€)
unique_words = list(dict.fromkeys(all_words))

# ì €ì¥
with open("atk_words.json", "w", encoding="utf-8") as f:
    json.dump(unique_words, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ì¤‘ë³µ ì œê±° í›„ {len(unique_words)}ê°œì˜ ë‹¨ì–´ë¥¼ atk_words.jsonì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
